{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "746626f6b18e1c8c",
   "metadata": {},
   "source": [
    "## Amazon AgentCore Bedrock Code Interpreter を使った高度なデータ分析 - チュートリアル (Strands)\n\nこのチュートリアルでは、Python を使ってコードを実行することで高度なデータ分析を行う AI エージェントを作成する方法を示します。LLM によって生成されたコードを実行するために、Amazon Bedrock AgentCore Code Interpreter を使用します。\n\nこのチュートリアルでは、AgentCore Bedrock Code Interpreter を使って以下のことを実演します。\n1. サンドボックス環境のセットアップ\n2. ユーザーのクエリに基づいてコードを生成し、高度なデータ分析を行うストランドベースのエージェントの構成\n3. Code Interpreter を使ってサンドボックス環境でコードを実行\n4. 結果をユーザーに表示\n\n## 前提条件\n- Bedrock AgentCore Code Interpreter にアクセスできる AWS アカウント\n- コードインタープリターリソースを作成および管理するための必要な IAM 許可\n- 必要な Python パッケージがインストールされている (boto3、bedrock-agentcore、strands を含む)\n- Amazon Bedrock でモデルを呼び出す許可を持つ IAM ロール\n - US オレゴン (us-west-2) リージョンの Claude 3.7 Sonnet モデルにアクセスできる (Strands SDK のデフォルトモデル)\n\n## IAM 実行ロールには、以下の IAM ポリシーが付与されている必要があります"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f323388415caf3f7",
   "metadata": {},
   "source": [
    "~~~ {\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n    {\n        \"Effect\": \"Allow\",\n        \"Action\": [\n            \"bedrock-agentcore:CreateCodeInterpreter\",\n            \"bedrock-agentcore:StartCodeInterpreterSession\",\n            \"bedrock-agentcore:InvokeCodeInterpreter\",\n            \"bedrock-agentcore:StopCodeInterpreterSession\",\n            \"bedrock-agentcore:DeleteCodeInterpreter\",\n            \"bedrock-agentcore:ListCodeInterpreters\",\n            \"bedrock-agentcore:GetCodeInterpreter\"\n        ],\n        \"Resource\": \"*\"\n    },\n    {\n        \"Effect\": \"Allow\",\n        \"Action\": [\n            \"logs:CreateLogGroup\",\n            \"logs:CreateLogStream\",\n            \"logs:PutLogEvents\"\n        ],\n        \"Resource\": \"arn:aws:logs:*:*:log-group:/aws/bedrock-agentcore/code-interpreter*\"\n    }\n]\n}\n\n日本語訳:\n\"バージョン\": \"2012-10-17\",\n\"ステートメント\": [\n    {\n        \"効果\": \"許可\",\n        \"アクション\": [\n            \"bedrock-agentcore:CreateCodeInterpreter\",\n            \"bedrock-agentcore:StartCodeInterpreterSession\",\n            \"bedrock-agentcore:InvokeCodeInterpreter\",\n            \"bedrock-agentcore:StopCodeInterpreterSession\",\n            \"bedrock-agentcore:DeleteCodeInterpreter\",\n            \"bedrock-agentcore:ListCodeInterpreters\",\n            \"bedrock-agentcore:GetCodeInterpreter\"\n        ],\n        \"リソース\": \"*\"\n    },\n    {\n        \"効果\": \"許可\", \n        \"アクション\": [\n            \"logs:CreateLogGroup\",\n            \"logs:CreateLogStream\",\n            \"logs:PutLogEvents\"\n        ],\n        \"リソース\": \"arn:aws:logs:*:*:log-group:/aws/bedrock-agentcore/code-interpreter*\"\n    }\n]\n~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226b2cb86ff18d9c",
   "metadata": {},
   "source": [
    "## 動作の仕組み\n\nコード実行サンドボックスは、コード インタプリター、シェル、ファイル システムを備えた分離された環境を作成することで、エージェントがユーザーのクエリを安全に処理できるようにします。大規模言語モデルがツールの選択を支援した後、コードがこのセッション内で実行され、その結果がユーザーまたはエージェントに合成のために返されます。\n\n![architecture local](code-interpreter.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859482709c77b03d",
   "metadata": {},
   "source": [
    "## 1. 環境のセットアップ\n\nまず、必要なライブラリをインポートし、Code Interpreter クライアントを初期化しましょう。\n\nデフォルトのセッションタイムアウトは 900 秒(15 分)です。しかし、データの詳細な分析を行うため、今回はセッションタイムアウト時間を 1200 秒(20 分)に長めに設定してセッションを開始します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13da423bac8ddd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "bb006310a96750c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T09:35:48.346322Z",
     "start_time": "2025-07-13T09:35:46.244470Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'01K01J7Z3DH445CJ0YW14BK2GY'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bedrock_agentcore.tools.code_interpreter_client import CodeInterpreter\nfrom strands import Agent, tool\nimport json\nimport pandas as pd\nfrom typing import Dict, Any, List\n\n# AWS の対応リージョン内で Code Interpreter を初期化します。\n\n日本語訳:\ncode_client = CodeInterpreter('us-west-2')\ncode_client.start(session_timeout_seconds=1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd02b57bd10dda2",
   "metadata": {},
   "source": [
    "## 2. ローカルデータファイルの読み込み\n\nここでは、サンプルデータファイルの内容を読み込みます。このファイルには、 Name 、 Preferred_City 、 Preferred_Animal 、 Preferred_Thing の 4 列があり、ランダムなデータが約 30 万件記録されています。\n\n後ほど、このファイルを agent を使って分析し、分布とアウトライアを把握します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "bef3821f290a589b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T09:35:48.465278Z",
     "start_time": "2025-07-13T09:35:48.385287Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Preferred_City</th>\n",
       "      <th>Preferred_Animal</th>\n",
       "      <th>Preferred_Thing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Betty Ramirez</td>\n",
       "      <td>Dallas</td>\n",
       "      <td>Elephant</td>\n",
       "      <td>Sofa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jennifer Green</td>\n",
       "      <td>Naples</td>\n",
       "      <td>Bee</td>\n",
       "      <td>Shirt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>John Lopez</td>\n",
       "      <td>Helsinki</td>\n",
       "      <td>Zebra</td>\n",
       "      <td>Wallet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Susan Gonzalez</td>\n",
       "      <td>Beijing</td>\n",
       "      <td>Chicken</td>\n",
       "      <td>Phone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jennifer Wright</td>\n",
       "      <td>Buenos Aires</td>\n",
       "      <td>Goat</td>\n",
       "      <td>Wallet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Name Preferred_City Preferred_Animal Preferred_Thing\n",
       "0    Betty Ramirez         Dallas         Elephant            Sofa\n",
       "1   Jennifer Green         Naples              Bee           Shirt\n",
       "2       John Lopez       Helsinki            Zebra          Wallet\n",
       "3   Susan Gonzalez        Beijing          Chicken           Phone\n",
       "4  Jennifer Wright   Buenos Aires             Goat          Wallet"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data = pd.read_csv(\"samples/data.csv\")\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38277480cbc38ba0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T09:35:48.503763Z",
     "start_time": "2025-07-13T09:35:48.495825Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_file(file_path: str) -> str:\n    \"\"\"ファイルの内容を読み込むためのヘルパー関数 (エラー処理付き)\"\"\"\n    try:\n        with open(file_path, 'r', encoding='utf-8') as file:\n            return file.read()\n    except FileNotFoundError:\n        print(f\"Error: The file '{file_path}' was not found.\")\n        return \"\"\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        return \"\"\n\ndata_file_content = read_file(\"samples/data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fc91e0fb83fa05",
   "metadata": {},
   "source": [
    "## 3. Sandbox 環境のためのファイルの準備\n\nSandbox 環境で作成したいファイルを定義する構造を作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "da44fb745b84c6ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T09:35:49.703849Z",
     "start_time": "2025-07-13T09:35:49.699079Z"
    }
   },
   "outputs": [],
   "source": [
    "files_to_create = [\n",
    "                {\n",
    "                    \"path\": \"data.csv\",\n",
    "                    \"text\": data_file_content\n",
    "                }]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f055bea34c93279",
   "metadata": {},
   "source": [
    "## 4. Creating Helper Function for Tool Invocation\n\nThis helper function will make it easier to call sandbox tools and handle their responses. Within an active session, you can execute code in supported languages (Python, JavaScript), access libraries based on your dependencies configuration, generate visualizations, and maintain state between executions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "a74164c54b3b8ad5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T09:35:50.359366Z",
     "start_time": "2025-07-13T09:35:50.356755Z"
    }
   },
   "outputs": [],
   "source": [
    "def call_tool(tool_name: str, arguments: Dict[str, Any]) -> Dict[str, Any]:\n    以下が日本語訳になります。\n\n\"\"\"サンドボックスツールを呼び出すためのヘルパー関数\n\n    引数:\n        tool_name (str): 呼び出すツールの名前\n        arguments (Dict[str, Any]): ツールに渡す引数\n\n    戻り値:\n        Dict[str, Any]: JSON 形式の結果\n    \"\"\"\n    response = code_client.invoke(tool_name, arguments)\n    for event in response[\"stream\"]:\n        return json.dumps(event[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd33790785ac084a",
   "metadata": {},
   "source": [
    "## 5. Code Sandbox にデータファイルを書き込む\n\n次に、データファイルをサンドボックス環境に書き込み、正常に作成されたことを確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "380afae3a5ba4934",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T09:36:00.346136Z",
     "start_time": "2025-07-13T09:35:50.965773Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing files result:\n",
      "{\"content\": [{\"type\": \"text\", \"text\": \"Successfully wrote all 1 files\"}], \"isError\": false}\n",
      "\n",
      "Files in sandbox:\n",
      "{\"content\": [{\"type\": \"resource_link\", \"uri\": \"file:///log\", \"name\": \"log\", \"description\": \"Directory\"}, {\"type\": \"resource_link\", \"mimeType\": \"text/csv\", \"uri\": \"file:///data.csv\", \"name\": \"data.csv\", \"description\": \"File\"}, {\"type\": \"resource_link\", \"uri\": \"file:///.ipython\", \"name\": \".ipython\", \"description\": \"Directory\"}], \"isError\": false}\n"
     ]
    }
   ],
   "source": [
    "# Write files to sandbox\n\n日本語訳:\n# サンドボックスにファイルを書き込む\n\nThe `write_file` function allows you to write data to a file in the sandbox. 指定した `path` にファイルが存在しない場合は新規作成され、存在する場合は上書きされます。\n\n```python\nimport sandbox\n\ndata = \"Hello World!\"\nsandbox.write_file(\"/path/to/hello.txt\", data)\n```\n\n`binary` フラグを `True` に設定すると、バイナリモードでファイルが開かれます。これは、バイナリデータ (画像など) を扱う場合に便利です。\n\n```python\nimport sandbox\n\nbinary_data = b\"\\x89\\x50\\x4e\\x47\\x0d\\x0a\\x1a\\x0a\"  # PNG ヘッダー\nsandbox.write_file(\"/path/to/image.png\", binary_data, binary=True)\n```\n\nファイルの内容は、`read_file` 関数を使って読み取ることができます。\nwriting_files = call_tool(\"writeFiles\", {\"content\": files_to_create})\nprint(\"Writing files result:\")\nprint(writing_files)\n\n# Verify files were created\n\nファイルが作成されたことを確認する\nlisting_files = call_tool(\"listFiles\", {\"path\": \"\"})\nprint(\"\\nFiles in sandbox:\")\nprint(listing_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640eae7a52ce9f8d",
   "metadata": {},
   "source": [
    "## 6. Strands ベースのエージェントを使用した高度な分析の実行\n\nここでは、サンドボックス (上記) にアップロードしたデータファイルに対してデータ分析を実行するエージェントを構成します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3b068faf4a5eaa",
   "metadata": {},
   "source": [
    "### 6.1 システムプロンプトの定義\nAI アシスタントの振る舞いと機能を定義します。私たちは、アシスタントに常にコードの実行とデータに基づく推論によって回答を検証するよう指示しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "6e6830a170b45ce3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T09:36:00.366216Z",
     "start_time": "2025-07-13T09:36:00.364374Z"
    }
   },
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = はい、以下のように翻訳しました。\n\n\"\"\"あなたは、提供されたツールを使ってコードの実行によりすべての回答を検証する、役立つAIアシスタントです。ツールを使わずに質問に答えないでください。\n\n検証の原則:\n1. コード、アルゴリズム、または計算について主張する場合は、それらを検証するコードを書きます\n2. execute_python を使って数学的な計算、アルゴリズム、論理を テストします\n3. 回答を出す前に、テストスクリプトを作成して理解度を検証します  \n4. 実際のコード実行で作業過程を必ず示します\n5. 不確かな場合は、制限を明示的に述べ、検証できる範囲を検証します\n\nアプローチ:\n- プログラミングの概念について尋ねられた場合は、それをコードで実装して示します\n- 計算を求められた場合は、プログラムで計算し、コードも示します\n- アルゴリズムを実装する場合は、正しさを証明するためのテストケースを含めます\n- 透明性を保つため、検証プロセスを文書化します  \n- サンドボックスでは実行間で状態が維持されるので、前の結果を参照できます\n\n使用可能なツール:\n- execute_python: Python コードを実行して出力を確認\n\n応答フォーマット: execute_pythonツールは以下のフィールドを含むJSONレスポンスを返します:\n- sessionId: サンドボックスセッションID\n- id: リクエストID \n- isError: エラーがあったかどうかを示すブール値\n- content: type と text/data を持つコンテンツオブジェクトの配列\n- structuredContent: コード実行の場合、stdout、stderr、exitCode、executionTimeが含まれます\n\nコード実行が成功した場合、出力は content[0].text と structuredContent.stdout の両方に表示されます。\nisError フィールドを確認して、エラーがあったかどうかを確認してください。\n\nできる限り徹底的で正確に、検証可能な場合は必ず回答を検証してください。\"\"\"\n\n技術的な用語はそのまま残し、半角英数字の前後に半角スペースを挿入しました。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87157a0ea835ab5",
   "metadata": {},
   "source": [
    "### 6.2 コード実行ツールの定義\n次に、エージェントがコードサンドボックスでコードを実行するためのツールとして使用する関数を定義します。 @tool デコレータを使って、関数をエージェントのカスタムツールとしてアノテートします。\n\nアクティブなコード インタープリター セッション内では、サポートされている言語 (Python、JavaScript) でコードを実行し、依存関係の構成に基づいてライブラリにアクセスし、可視化を生成し、実行間で状態を維持できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "750472cd96e873c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T09:36:34.464620Z",
     "start_time": "2025-07-13T09:36:34.457484Z"
    }
   },
   "outputs": [],
   "source": [
    "#Define and configure the code interpreter tool\n\n#コード解釈ツールを定義および構成する\n\n日本語訳:\n\nコード 解釈ツールを 定義 および 構成する ことを 説明します 。 半角英数字 の 前後 には 半角スペース を 挿入し 、 コード 、 コマンド 、 変数名 、 関数名 など の 技術的 な 用語 は そのまま 残します 。\n@tool\ndef execute_python(code: str, description: str = \"\") -> str:\n    翻訳するテキスト:\n\"\"\"Execute Python code in the sandbox.\"\"\"\n\n日本語訳:\n\"\"\"サンドボックス内で Python コードを実行します。\"\"\"\n\n    if description:\n        code = f\"# {description}の説明\\n{code}のコード\n\n    #Print generated Code to be executed\n\n生成されたコードを出力して実行します。\n    print(f\"\\n Generated Code: {code}\")\n\n\n    # Call the Invoke メソッドを呼び出し、初期化されたコード インタープリター セッション内で生成されたコードを実行する\n\n日本語訳:\n# Invoke メソッドを呼び出し、初期化されたコードインタプリターセッション内で生成されたコードを実行します\n    response = code_client.invoke(\"executeCode\", {\n        \"code\": code,\n        \"language\": \"python\",\n        \"clearContext\": False\n    })\n    for event in response[\"stream\"]:\n        return json.dumps(event[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bc47b82755730d",
   "metadata": {},
   "source": [
    "### 6.3 エージェントの設定\nStrands SDK を使用してエージェントを作成し、設定します。上で定義したシステムプロンプトとツールを提供し、コード生成を実行します。\n\n日本語訳:\n### 6.3 エージェントの設定\nStrands SDK を使用して エージェント を作成し、設定します。システムプロンプト と、上で定義した ツール を提供し、コード生成 を実行します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "14c5e8f18b70dc01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T09:36:35.341080Z",
     "start_time": "2025-07-13T09:36:35.239620Z"
    }
   },
   "outputs": [],
   "source": [
    "#configure the strands agent including the tool(s)\n\n日本語訳:\n\n# strands エージェントとツール (s) を構成します。\nagent=Agent(\n        tools=[execute_python],\n        system_prompt=SYSTEM_PROMPT,\n        callback_handler=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25693e10aa1e5689",
   "metadata": {},
   "source": [
    "## 7. エージェントの呼び出しと応答処理\n私たちはクエリでエージェントを呼び出し、エージェントの応答を処理します\n\n注意: 非同期実行には非同期環境での実行が必要です"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddfa7cb97ead950",
   "metadata": {},
   "source": [
    "## 7.1 探索的データ分析 (EDA) を実行するためのクエリ\n\n日本語訳:\n\n探索的データ分析 (EDA) は、データセットの特性を理解するための重要なステップです。 SQL を使用して EDA を実行することができます。以下は、 EDA を実行するための一般的な SQL クエリの例です。\n\nデータの概要を取得するには、 `SELECT COUNT(*) FROM table_name;` を使用して行数を確認します。\n\n列の値の分布を確認するには、 `SELECT column_name, COUNT(*) FROM table_name GROUP BY column_name;` を使用します。\n\n欠損値の有無を確認するには、 `SELECT SUM(CASE WHEN column_name IS NULL THEN 1 ELSE 0 END) AS null_count FROM table_name;` を使用します。\n\n数値データの概要統計量 (最小値、最大値、平均値、中央値など) を取得するには、 `SELECT MIN(column_name), MAX(column_name), AVG(column_name), MEDIAN(column_name) FROM table_name;` を使用します。\n\nカテゴリデータの一意の値とその出現回数を確認するには、 `SELECT column_name, COUNT(*) FROM table_name GROUP BY column_name;` を使用します。\n\nこれらのクエリは、データセットの構造と内容を理解するのに役立ちます。探索的データ分析の結果に基づいて、データの前処理や特徴量エンジニアリングの必要性を判断できます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f98916e2cc3627",
   "metadata": {},
   "source": [
    "以下が日本語訳になります。\n\nコード サンドボックス 環境のデータ ファイルに対して探索的データ分析を実行するよう エージェントに指示するクエリから始めましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "7370ff964d06a1cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T09:40:07.612284Z",
     "start_time": "2025-07-13T09:37:28.402310Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'll perform an exploratory data analysis (EDA) on the data.csv file to examine distributions and identify any outliers. Let me work through this step by step.I see we're missing some libraries. Let me try without seaborn and use the standard libraries:Now let's analyze each categorical column in more detail:Let's check for patterns in preferences and analyze distributions visually:Let's check for outliers and get more detailed statistics:Let's further explore interesting patterns and relationships in the data:Based on the comprehensive exploratory data analysis (EDA) of the data.csv file, here's a summary of the findings:\n",
      "\n",
      "## 1. Dataset Overview\n",
      "\n",
      "- **Size**: The dataset contains 299,130 records with 4 columns: Name, Preferred_City, Preferred_Animal, and Preferred_Thing.\n",
      "- **Data types**: All columns contain categorical (object) data.\n",
      "- **Unique values**:\n",
      "  - 1,722 unique names (combinations of first and last names)\n",
      "  - 55 unique cities\n",
      "  - 50 unique animals\n",
      "  - 51 unique things\n",
      "\n",
      "## 2. Distributions\n",
      "\n",
      "### Names\n",
      "- There are 42 unique first names and 41 unique last names in the dataset.\n",
      "- First name distribution:\n",
      "  - Mean: 7,122.14 occurrences per name\n",
      "  - Most common: \"Dorothy\" (7,264 occurrences)\n",
      "  - Least common: \"Donna\" (6,952 occurrences)\n",
      "  - Standard deviation: 74.69\n",
      "\n",
      "### Cities\n",
      "- City distribution:\n",
      "  - Mean: 5,438.73 occurrences per city\n",
      "  - Most common: \"Prague\" (5,587 occurrences)\n",
      "  - Least common: \"Phoenix\" (5,236 occurrences)\n",
      "  - Standard deviation: 86.88 (1.60% of the mean)\n",
      "\n",
      "### Animals\n",
      "- Animal distribution:\n",
      "  - Mean: 5,982.60 occurrences per animal\n",
      "  - Most common: \"Goat\" (6,141 occurrences)\n",
      "  - Least common: \"Shark\" (5,761 occurrences)\n",
      "  - Standard deviation: 86.54 (1.45% of the mean)\n",
      "\n",
      "### Things\n",
      "- Thing distribution:\n",
      "  - Mean: 5,865.29 occurrences per thing\n",
      "  - Most common: \"Pencil\" (6,058 occurrences)\n",
      "  - Least common: \"Candle\" (5,720 occurrences)\n",
      "  - Standard deviation: 76.11 (1.30% of the mean)\n",
      "\n",
      "## 3. Outliers\n",
      "\n",
      "- Using the Z-score method (threshold = 3), no outliers were detected in any category.\n",
      "- Using the IQR method:\n",
      "  - Cities: No outliers detected\n",
      "  - Animals: \"Shark\" is identified as an outlier with 5,761 occurrences (below the lower bound of 5,779.25)\n",
      "  - Things: No outliers detected\n",
      "\n",
      "## 4. Patterns and Relationships\n",
      "\n",
      "### Name Duplication\n",
      "- All names appear multiple times in the dataset\n",
      "- The most common name is \"Lisa White\" which appears 222 times\n",
      "- People with the same name have different preferences\n",
      "\n",
      "### Preference Distributions\n",
      "- The distributions of preferences for cities, animals, and things are remarkably uniform\n",
      "- The small standard deviations relative to means (1.3%-1.6%) indicate very even distributions\n",
      "- This suggests the data might be synthetically generated with minor random variations\n",
      "\n",
      "### Independence Analysis\n",
      "- First names and city preferences show weak association:\n",
      "  - Each first name's most preferred city represents only 2.07%-2.36% of people with that name\n",
      "  - E.g., \"Dorothy\" prefers \"Florence\" but only 2.26% of Dorothys chose Florence\n",
      "\n",
      "### Common Combinations\n",
      "- Most common city-animal combination: Amsterdam-Pig (151 occurrences)\n",
      "- Most common animal-thing combination: Cat-Basket (151 occurrences)\n",
      "\n",
      "### Gender Analysis (Inferred from Names)\n",
      "- Slight differences in preferences between inferred genders:\n",
      "  - Female: prefer Mumbai, Atlanta, Rome cities; Tiger, Owl, Pig animals; Can, Game, Knife things\n",
      "  - Male: prefer Athens, Seoul, Venice cities; Penguin, Whale, Guinea Pig animals; Bowl, Box, Basket things\n",
      "\n",
      "## 5. Key Insights\n",
      "\n",
      "1. **Highly Uniform Data**: The remarkably even distribution across all categories suggests this might be synthetic data generated with slight randomization.\n",
      "\n",
      "2. **Weak Associations**: There appears to be little meaningful association between name and preferences, with the top preferences for each name occurring at frequencies close to what random chance would predict.\n",
      "\n",
      "3. **Minimal Outliers**: Only one outlier was detected (Shark as a preferred animal) using the IQR method, and none using Z-score, further suggesting careful data generation.\n",
      "\n",
      "4. **Name Duplication**: Each full name appears multiple times but with different preferences, indicating the dataset doesn't use name as a unique identifier.\n",
      "\n",
      "The data appears to be synthetically generated with preferences assigned in a mostly random but controlled manner to create a very uniform distribution with slight variations. The small percentage differences in preference associations are likely due to random variations rather than meaningful patterns."
     ]
    }
   ],
   "source": [
    "query = \"Perform exploratory data analysis(EDA) on the file 'data.csv'. Tell me about distributions and outlier values.\"\n\n# Invoke the agent asynchcronously and stream the response\n\nエージェントを非同期で呼び出し、レスポンスをストリーミングする\nresponse_text = \"\"\nasync for event in agent.stream_async(query):\n    if \"data\" in event:\n        # Stream text response\n\n以下のように翻訳します。\n\n# テキストレスポンスをストリーミング\n\n半角英数字の前後に半角スペースを挿入し、コード、コマンド、変数名、関数名などの技術的な用語はそのまま残しました。\n        chunk = event[\"data\"]\n        response_text += chunk\n        print(chunk, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accab0cfe53ade15",
   "metadata": {},
   "source": [
    "## 7.2 情報を抽出するクエリ\n\nさて、コード サンドボックス環境のデータファイルから特定の情報を抽出するよう、エージェントに指示しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "f091d1f87558bd41",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T09:37:07.283968Z",
     "start_time": "2025-07-13T09:36:45.865171Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'll help you analyze the data.csv file to find individuals named 'Kimberly' who have 'Crocodile' as their favorite animal. Let me use Python to check this data file.\n",
      " Generated Code: # Checking if data.csv exists and examining its structure\n",
      "# First, let's check if the file exists\n",
      "import os\n",
      "import pandas as pd\n",
      "\n",
      "# Check if the file exists\n",
      "if os.path.exists('data.csv'):\n",
      "    print(f\"File 'data.csv' exists. Reading the file...\")\n",
      "    # Try to read the CSV file\n",
      "    try:\n",
      "        df = pd.read_csv('data.csv')\n",
      "        print(\"File successfully loaded. Here's a preview:\")\n",
      "        print(df.head())\n",
      "        print(f\"\\nTotal rows in the file: {len(df)}\")\n",
      "        print(f\"Columns in the file: {df.columns.tolist()}\")\n",
      "    except Exception as e:\n",
      "        print(f\"Error reading the file: {e}\")\n",
      "else:\n",
      "    print(f\"File 'data.csv' does not exist in the current directory.\")\n",
      "    print(f\"Current directory contents: {os.listdir()}\")\n",
      "Great! The data.csv file exists and has been successfully loaded. Now I can see it has 299,130 rows and includes the columns: 'Name', 'Preferred_City', 'Preferred_Animal', and 'Preferred_Thing'.\n",
      "\n",
      "Let me now analyze the data to find individuals with the first name 'Kimberly' who have 'Crocodile' as their favorite animal:\n",
      " Generated Code: # Counting individuals named Kimberly with Crocodile as favorite animal\n",
      "import pandas as pd\n",
      "\n",
      "# Read the data\n",
      "df = pd.read_csv('data.csv')\n",
      "\n",
      "# Extract first name from the full name\n",
      "df['First_Name'] = df['Name'].str.split().str[0]\n",
      "\n",
      "# Filter for individuals with first name 'Kimberly' and 'Crocodile' as favorite animal\n",
      "filtered_df = df[(df['First_Name'] == 'Kimberly') & (df['Preferred_Animal'] == 'Crocodile')]\n",
      "\n",
      "# Get the count and display results\n",
      "count = len(filtered_df)\n",
      "print(f\"Number of individuals named Kimberly with Crocodile as favorite animal: {count}\")\n",
      "\n",
      "# If there are any matches, show a few examples\n",
      "if count > 0:\n",
      "    print(\"\\nExample records:\")\n",
      "    print(filtered_df.head(min(5, count)))\n",
      "Based on the analysis of the data.csv file, there are **120 individuals** with the first name 'Kimberly' who have 'Crocodile' as their favorite animal.\n",
      "\n",
      "The code extracted the first name from the 'Name' column and then filtered the data to find records where:\n",
      "1. The first name is 'Kimberly'\n",
      "2. The preferred animal is 'Crocodile'\n",
      "\n",
      "The analysis shows examples of these individuals, including their full names, preferred cities, and preferred things. This confirms the answer of 120 individuals.\n",
      " Generated Code: # Basic information about the dataset\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "\n",
      "# Load the dataset\n",
      "df = pd.read_csv('data.csv')\n",
      "\n",
      "# Basic information\n",
      "print(\"Dataset Shape:\", df.shape)\n",
      "print(\"\\nData Types:\")\n",
      "print(df.dtypes)\n",
      "print(\"\\nFirst 5 rows:\")\n",
      "print(df.head())\n",
      "print(\"\\nSummary Statistics:\")\n",
      "print(df.describe(include='all'))\n",
      "\n",
      " Generated Code: # Basic information about the dataset\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# Load the dataset\n",
      "df = pd.read_csv('data.csv')\n",
      "\n",
      "# Basic information\n",
      "print(\"Dataset Shape:\", df.shape)\n",
      "print(\"\\nData Types:\")\n",
      "print(df.dtypes)\n",
      "print(\"\\nFirst 5 rows:\")\n",
      "print(df.head())\n",
      "print(\"\\nSummary Statistics:\")\n",
      "print(df.describe(include='all'))\n",
      "\n",
      " Generated Code: # Analyzing distributions of categorical variables\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Load the dataset\n",
      "df = pd.read_csv('data.csv')\n",
      "\n",
      "# Extract first name from the full name\n",
      "df['First_Name'] = df['Name'].str.split().str[0]\n",
      "df['Last_Name'] = df['Name'].str.split().str[1]\n",
      "\n",
      "# Display most common first names\n",
      "print(\"Top 10 Most Common First Names:\")\n",
      "print(df['First_Name'].value_counts().head(10))\n",
      "print(\"\\nLeast Common First Names:\")\n",
      "print(df['First_Name'].value_counts().tail(5))\n",
      "\n",
      "# Display most common last names\n",
      "print(\"\\nTop 10 Most Common Last Names:\")\n",
      "print(df['Last_Name'].value_counts().head(10))\n",
      "\n",
      "# Display most common cities\n",
      "print(\"\\nTop 10 Most Common Preferred Cities:\")\n",
      "print(df['Preferred_City'].value_counts().head(10))\n",
      "print(\"\\nLeast Common Preferred Cities:\")\n",
      "print(df['Preferred_City'].value_counts().tail(5))\n",
      "\n",
      "# Display most common animals\n",
      "print(\"\\nTop 10 Most Common Preferred Animals:\")\n",
      "print(df['Preferred_Animal'].value_counts().head(10))\n",
      "print(\"\\nLeast Common Preferred Animals:\")\n",
      "print(df['Preferred_Animal'].value_counts().tail(5))\n",
      "\n",
      "# Display most common things\n",
      "print(\"\\nTop 10 Most Common Preferred Things:\")\n",
      "print(df['Preferred_Thing'].value_counts().head(10))\n",
      "print(\"\\nLeast Common Preferred Things:\")\n",
      "print(df['Preferred_Thing'].value_counts().tail(5))\n",
      "\n",
      " Generated Code: # Creating histograms and analyzing distributions\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Load the dataset\n",
      "df = pd.read_csv('data.csv')\n",
      "\n",
      "# Setting figure size\n",
      "plt.figure(figsize=(10, 6))\n",
      "\n",
      "# Distribution of preferences counts\n",
      "name_counts = df['Name'].value_counts()\n",
      "city_counts = df['Preferred_City'].value_counts()\n",
      "animal_counts = df['Preferred_Animal'].value_counts()\n",
      "thing_counts = df['Preferred_Thing'].value_counts()\n",
      "\n",
      "# Calculate statistics for distribution analysis\n",
      "print(\"=== Distribution Statistics ===\")\n",
      "print(\"\\nPreferred City Distribution:\")\n",
      "print(f\"Mean count per city: {city_counts.mean():.2f}\")\n",
      "print(f\"Median count per city: {city_counts.median():.2f}\")\n",
      "print(f\"Min count: {city_counts.min()} (City: {city_counts.idxmin()})\")\n",
      "print(f\"Max count: {city_counts.max()} (City: {city_counts.idxmax()})\")\n",
      "print(f\"Standard deviation: {city_counts.std():.2f}\")\n",
      "\n",
      "print(\"\\nPreferred Animal Distribution:\")\n",
      "print(f\"Mean count per animal: {animal_counts.mean():.2f}\")\n",
      "print(f\"Median count per animal: {animal_counts.median():.2f}\")\n",
      "print(f\"Min count: {animal_counts.min()} (Animal: {animal_counts.idxmin()})\")\n",
      "print(f\"Max count: {animal_counts.max()} (Animal: {animal_counts.idxmax()})\")\n",
      "print(f\"Standard deviation: {animal_counts.std():.2f}\")\n",
      "\n",
      "print(\"\\nPreferred Thing Distribution:\")\n",
      "print(f\"Mean count per thing: {thing_counts.mean():.2f}\")\n",
      "print(f\"Median count per thing: {thing_counts.median():.2f}\")\n",
      "print(f\"Min count: {thing_counts.min()} (Thing: {thing_counts.idxmin()})\")\n",
      "print(f\"Max count: {thing_counts.max()} (Thing: {thing_counts.idxmax()})\")\n",
      "print(f\"Standard deviation: {thing_counts.std():.2f}\")\n",
      "\n",
      "# Check for name uniqueness and distribution\n",
      "name_uniqueness = len(df['Name'].unique())\n",
      "name_dup_count = df['Name'].duplicated().sum()\n",
      "print(f\"\\nUnique names: {name_uniqueness}\")\n",
      "print(f\"Duplicated names: {name_dup_count}\")\n",
      "print(f\"Most common name appears {name_counts.max()} times: {name_counts.idxmax()}\")\n",
      "\n",
      "# Check for uniform distribution in data\n",
      "print(\"\\n=== Testing for Uniform Distribution ===\")\n",
      "# If distribution is uniform, we expect similar counts for each value\n",
      "print(\"Standard deviation as percentage of mean:\")\n",
      "print(f\"Cities: {(city_counts.std()/city_counts.mean())*100:.2f}%\")\n",
      "print(f\"Animals: {(animal_counts.std()/animal_counts.mean())*100:.2f}%\")\n",
      "print(f\"Things: {(thing_counts.std()/thing_counts.mean())*100:.2f}%\")\n",
      "\n",
      "# Examine pairwise relationships for common combinations\n",
      "print(\"\\n=== Top Combinations ===\")\n",
      "print(\"Top 5 City-Animal Combinations:\")\n",
      "print(df.groupby(['Preferred_City', 'Preferred_Animal']).size().sort_values(ascending=False).head(5))\n",
      "\n",
      "print(\"\\nTop 5 Animal-Thing Combinations:\")\n",
      "print(df.groupby(['Preferred_Animal', 'Preferred_Thing']).size().sort_values(ascending=False).head(5))\n",
      "\n",
      "# Check for interesting correlations - if people with the same first name prefer similar things\n",
      "print(\"\\n=== First Name Preference Patterns ===\")\n",
      "first_names = df['Name'].str.split().str[0]\n",
      "top_first_names = first_names.value_counts().head(5).index\n",
      "\n",
      "for name in top_first_names:\n",
      "    name_filter = df['Name'].str.startswith(name + ' ')\n",
      "    print(f\"\\nTop preferences for people with first name '{name}':\")\n",
      "    print(f\"Top cities: {df[name_filter]['Preferred_City'].value_counts().head(3).to_dict()}\")\n",
      "    print(f\"Top animals: {df[name_filter]['Preferred_Animal'].value_counts().head(3).to_dict()}\")\n",
      "    print(f\"Top things: {df[name_filter]['Preferred_Thing'].value_counts().head(3).to_dict()}\")\n",
      "\n",
      " Generated Code: # Checking for outliers in the frequency distributions\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy import stats\n",
      "\n",
      "# Load the dataset\n",
      "df = pd.read_csv('data.csv')\n",
      "\n",
      "# Get value counts for each categorical column\n",
      "city_counts = df['Preferred_City'].value_counts()\n",
      "animal_counts = df['Preferred_Animal'].value_counts()\n",
      "thing_counts = df['Preferred_Thing'].value_counts()\n",
      "\n",
      "# Function to identify outliers using Z-score method\n",
      "def find_outliers_zscore(data, threshold=3):\n",
      "    z_scores = stats.zscore(data)\n",
      "    outliers = data[np.abs(z_scores) > threshold]\n",
      "    return outliers, z_scores\n",
      "\n",
      "# Apply outlier detection\n",
      "print(\"=== Outlier Detection (Z-score method) ===\")\n",
      "\n",
      "# Cities\n",
      "city_outliers, city_zscores = find_outliers_zscore(city_counts.values)\n",
      "if len(city_outliers) > 0:\n",
      "    print(\"\\nOutlier cities (by frequency):\")\n",
      "    for outlier in city_outliers:\n",
      "        city = city_counts[city_counts == outlier].index[0]\n",
      "        z = city_zscores[city_counts.values == outlier][0]\n",
      "        print(f\"{city}: {outlier} occurrences (z-score: {z:.2f})\")\n",
      "else:\n",
      "    print(\"\\nNo outlier cities detected\")\n",
      "\n",
      "# Animals\n",
      "animal_outliers, animal_zscores = find_outliers_zscore(animal_counts.values)\n",
      "if len(animal_outliers) > 0:\n",
      "    print(\"\\nOutlier animals (by frequency):\")\n",
      "    for outlier in animal_outliers:\n",
      "        animal = animal_counts[animal_counts == outlier].index[0]\n",
      "        z = animal_zscores[animal_counts.values == outlier][0]\n",
      "        print(f\"{animal}: {outlier} occurrences (z-score: {z:.2f})\")\n",
      "else:\n",
      "    print(\"\\nNo outlier animals detected\")\n",
      "\n",
      "# Things\n",
      "thing_outliers, thing_zscores = find_outliers_zscore(thing_counts.values)\n",
      "if len(thing_outliers) > 0:\n",
      "    print(\"\\nOutlier things (by frequency):\")\n",
      "    for outlier in thing_outliers:\n",
      "        thing = thing_counts[thing_counts == outlier].index[0]\n",
      "        z = thing_zscores[thing_counts.values == outlier][0]\n",
      "        print(f\"{thing}: {outlier} occurrences (z-score: {z:.2f})\")\n",
      "else:\n",
      "    print(\"\\nNo outlier things detected\")\n",
      "\n",
      "# Generate visualizations for distributions\n",
      "plt.figure(figsize=(15, 10))\n",
      "\n",
      "# City distribution\n",
      "plt.subplot(3, 1, 1)\n",
      "plt.hist(city_counts.values, bins=20)\n",
      "plt.title('Distribution of City Frequencies')\n",
      "plt.xlabel('Frequency')\n",
      "plt.ylabel('Count')\n",
      "\n",
      "# Animal distribution\n",
      "plt.subplot(3, 1, 2)\n",
      "plt.hist(animal_counts.values, bins=20)\n",
      "plt.title('Distribution of Animal Frequencies')\n",
      "plt.xlabel('Frequency')\n",
      "plt.ylabel('Count')\n",
      "\n",
      "# Thing distribution\n",
      "plt.subplot(3, 1, 3)\n",
      "plt.hist(thing_counts.values, bins=20)\n",
      "plt.title('Distribution of Thing Frequencies')\n",
      "plt.xlabel('Frequency')\n",
      "plt.ylabel('Count')\n",
      "\n",
      "plt.tight_layout()\n",
      "plt.savefig('distributions.png')\n",
      "\n",
      "# Calculate IQR for each distribution to identify outliers using another method\n",
      "def find_outliers_iqr(data):\n",
      "    Q1 = np.percentile(data, 25)\n",
      "    Q3 = np.percentile(data, 75)\n",
      "    IQR = Q3 - Q1\n",
      "    lower_bound = Q1 - 1.5 * IQR\n",
      "    upper_bound = Q3 + 1.5 * IQR\n",
      "    outliers = data[(data < lower_bound) | (data > upper_bound)]\n",
      "    return outliers, lower_bound, upper_bound, Q1, Q3, IQR\n",
      "\n",
      "print(\"\\n=== Outlier Detection (IQR method) ===\")\n",
      "\n",
      "# Cities\n",
      "city_outliers_iqr, lb_city, ub_city, Q1_city, Q3_city, IQR_city = find_outliers_iqr(city_counts.values)\n",
      "print(\"\\nCity Distribution:\")\n",
      "print(f\"Q1: {Q1_city:.2f}, Q3: {Q3_city:.2f}, IQR: {IQR_city:.2f}\")\n",
      "print(f\"Lower bound: {lb_city:.2f}, Upper bound: {ub_city:.2f}\")\n",
      "if len(city_outliers_iqr) > 0:\n",
      "    print(\"Outlier cities (IQR method):\")\n",
      "    for outlier in city_outliers_iqr:\n",
      "        city = city_counts[city_counts == outlier].index[0]\n",
      "        print(f\"{city}: {outlier} occurrences\")\n",
      "else:\n",
      "    print(\"No outlier cities detected with IQR method\")\n",
      "\n",
      "# Animals\n",
      "animal_outliers_iqr, lb_animal, ub_animal, Q1_animal, Q3_animal, IQR_animal = find_outliers_iqr(animal_counts.values)\n",
      "print(\"\\nAnimal Distribution:\")\n",
      "print(f\"Q1: {Q1_animal:.2f}, Q3: {Q3_animal:.2f}, IQR: {IQR_animal:.2f}\")\n",
      "print(f\"Lower bound: {lb_animal:.2f}, Upper bound: {ub_animal:.2f}\")\n",
      "if len(animal_outliers_iqr) > 0:\n",
      "    print(\"Outlier animals (IQR method):\")\n",
      "    for outlier in animal_outliers_iqr:\n",
      "        animal = animal_counts[animal_counts == outlier].index[0]\n",
      "        print(f\"{animal}: {outlier} occurrences\")\n",
      "else:\n",
      "    print(\"No outlier animals detected with IQR method\")\n",
      "\n",
      "# Things\n",
      "thing_outliers_iqr, lb_thing, ub_thing, Q1_thing, Q3_thing, IQR_thing = find_outliers_iqr(thing_counts.values)\n",
      "print(\"\\nThing Distribution:\")\n",
      "print(f\"Q1: {Q1_thing:.2f}, Q3: {Q3_thing:.2f}, IQR: {IQR_thing:.2f}\")\n",
      "print(f\"Lower bound: {lb_thing:.2f}, Upper bound: {ub_thing:.2f}\")\n",
      "if len(thing_outliers_iqr) > 0:\n",
      "    print(\"Outlier things (IQR method):\")\n",
      "    for outlier in thing_outliers_iqr:\n",
      "        thing = thing_counts[thing_counts == outlier].index[0]\n",
      "        print(f\"{thing}: {outlier} occurrences\")\n",
      "else:\n",
      "    print(\"No outlier things detected with IQR method\")\n",
      "\n",
      "print(\"\\nPlot saved as 'distributions.png'\")\n",
      "\n",
      "# Combine first and last names to check uniqueness\n",
      "df['First_Name'] = df['Name'].str.split().str[0]\n",
      "df['Last_Name'] = df['Name'].str.split().str[1]\n",
      "\n",
      "# Check for the total number of unique first and last names\n",
      "unique_first_names = df['First_Name'].nunique()\n",
      "unique_last_names = df['Last_Name'].nunique()\n",
      "\n",
      "print(f\"\\nUnique first names: {unique_first_names}\")\n",
      "print(f\"Unique last names: {unique_last_names}\")\n",
      "\n",
      "# Check if first and last names follow a uniform distribution\n",
      "first_name_counts = df['First_Name'].value_counts()\n",
      "last_name_counts = df['Last_Name'].value_counts()\n",
      "\n",
      "print(\"\\nFirst Name Distribution:\")\n",
      "print(f\"Mean: {first_name_counts.mean():.2f}\")\n",
      "print(f\"Median: {first_name_counts.median():.2f}\")\n",
      "print(f\"Min: {first_name_counts.min()} (Name: {first_name_counts.idxmin()})\")\n",
      "print(f\"Max: {first_name_counts.max()} (Name: {first_name_counts.idxmax()})\")\n",
      "print(f\"Std Dev: {first_name_counts.std():.2f}\")\n",
      "\n",
      "print(\"\\nLast Name Distribution:\")\n",
      "print(f\"Mean: {last_name_counts.mean():.2f}\")\n",
      "print(f\"Median: {last_name_counts.median():.2f}\")\n",
      "print(f\"Min: {last_name_counts.min()} (Name: {last_name_counts.idxmin()})\")\n",
      "print(f\"Max: {last_name_counts.max()} (Name: {last_name_counts.idxmax()})\")\n",
      "print(f\"Std Dev: {last_name_counts.std():.2f}\")\n",
      "\n",
      " Generated Code: # Analyzing patterns and conditional preferences\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "\n",
      "# Load the dataset\n",
      "df = pd.read_csv('data.csv')\n",
      "df['First_Name'] = df['Name'].str.split().str[0]\n",
      "df['Last_Name'] = df['Name'].str.split().str[1]\n",
      "\n",
      "# Check overall independence between variables\n",
      "print(\"=== Analyzing Independence Between Variables ===\")\n",
      "\n",
      "# Check for associations between first names and preferred cities\n",
      "print(\"\\nTop City for Each Common First Name:\")\n",
      "common_names = df['First_Name'].value_counts().head(10).index\n",
      "for name in common_names:\n",
      "    top_city = df[df['First_Name'] == name]['Preferred_City'].value_counts().idxmax()\n",
      "    city_count = df[(df['First_Name'] == name) & (df['Preferred_City'] == top_city)].shape[0]\n",
      "    total_name_count = df[df['First_Name'] == name].shape[0]\n",
      "    percentage = (city_count / total_name_count) * 100\n",
      "    print(f\"{name}: {top_city} ({city_count} out of {total_name_count}, {percentage:.2f}%)\")\n",
      "\n",
      "# Check for patterns in animal preferences\n",
      "print(\"\\nTop Animal for Each Common City:\")\n",
      "common_cities = df['Preferred_City'].value_counts().head(10).index\n",
      "for city in common_cities:\n",
      "    top_animal = df[df['Preferred_City'] == city]['Preferred_Animal'].value_counts().idxmax()\n",
      "    animal_count = df[(df['Preferred_City'] == city) & (df['Preferred_Animal'] == top_animal)].shape[0]\n",
      "    total_city_count = df[df['Preferred_City'] == city].shape[0]\n",
      "    percentage = (animal_count / total_city_count) * 100\n",
      "    print(f\"{city}: {top_animal} ({animal_count} out of {total_city_count}, {percentage:.2f}%)\")\n",
      "\n",
      "# Create a contingency table for Name-City pairs\n",
      "print(\"\\nContingency Analysis:\")\n",
      "# Sample a subset to keep output manageable\n",
      "name_sample = df['First_Name'].value_counts().head(5).index\n",
      "city_sample = df['Preferred_City'].value_counts().head(5).index\n",
      "\n",
      "# Filter to only these popular names and cities\n",
      "sample_df = df[df['First_Name'].isin(name_sample) & df['Preferred_City'].isin(city_sample)]\n",
      "contingency = pd.crosstab(sample_df['First_Name'], sample_df['Preferred_City'])\n",
      "print(contingency)\n",
      "\n",
      "# Calculate expected values if variables were independent\n",
      "row_totals = contingency.sum(axis=1).values.reshape(-1, 1)\n",
      "col_totals = contingency.sum(axis=0).values.reshape(1, -1)\n",
      "total = contingency.values.sum()\n",
      "expected = np.dot(row_totals, col_totals) / total\n",
      "expected_df = pd.DataFrame(expected, index=contingency.index, columns=contingency.columns)\n",
      "\n",
      "print(\"\\nExpected frequencies if independent:\")\n",
      "print(expected_df.round(2))\n",
      "\n",
      "# Display differences between observed and expected\n",
      "print(\"\\nDifference (Observed - Expected):\")\n",
      "diff = contingency - expected_df\n",
      "print(diff.round(2))\n",
      "\n",
      "# Check for distribution of preferences across genders (inferring from first names)\n",
      "# Note: This is a simplistic approach for analysis purposes only\n",
      "typical_female_names = ['Dorothy', 'Sarah', 'Elizabeth', 'Nancy', 'Jessica', 'Michelle', 'Jennifer']\n",
      "typical_male_names = ['Mark', 'Joseph', 'Charles', 'Christopher', 'David', 'Richard', 'Thomas']\n",
      "\n",
      "# Create a simple gender column\n",
      "df['Inferred_Gender'] = 'Unknown'\n",
      "df.loc[df['First_Name'].isin(typical_female_names), 'Inferred_Gender'] = 'Female'\n",
      "df.loc[df['First_Name'].isin(typical_male_names), 'Inferred_Gender'] = 'Male'\n",
      "\n",
      "# Analyze preferences by inferred gender\n",
      "print(\"\\n=== Analysis by Inferred Gender ===\")\n",
      "print(f\"Records with inferred gender: {df[df['Inferred_Gender'] != 'Unknown'].shape[0]}\")\n",
      "print(f\"Female-identified records: {df[df['Inferred_Gender'] == 'Female'].shape[0]}\")\n",
      "print(f\"Male-identified records: {df[df['Inferred_Gender'] == 'Male'].shape[0]}\")\n",
      "\n",
      "print(\"\\nTop 3 City Preferences by Gender:\")\n",
      "for gender in ['Female', 'Male']:\n",
      "    top_cities = df[df['Inferred_Gender'] == gender]['Preferred_City'].value_counts().head(3)\n",
      "    print(f\"{gender}: {dict(zip(top_cities.index, top_cities.values))}\")\n",
      "\n",
      "print(\"\\nTop 3 Animal Preferences by Gender:\")\n",
      "for gender in ['Female', 'Male']:\n",
      "    top_animals = df[df['Inferred_Gender'] == gender]['Preferred_Animal'].value_counts().head(3)\n",
      "    print(f\"{gender}: {dict(zip(top_animals.index, top_animals.values))}\")\n",
      "\n",
      "print(\"\\nTop 3 Thing Preferences by Gender:\")\n",
      "for gender in ['Female', 'Male']:\n",
      "    top_things = df[df['Inferred_Gender'] == gender]['Preferred_Thing'].value_counts().head(3)\n",
      "    print(f\"{gender}: {dict(zip(top_things.index, top_things.values))}\")\n",
      "\n",
      "# Check for name duplications and what that means for the data\n",
      "name_duplication = df['Name'].value_counts()\n",
      "print(\"\\n=== Analyzing Name Duplication ===\")\n",
      "print(f\"Number of unique full names: {len(name_duplication)}\")\n",
      "print(f\"Names appearing exactly once: {(name_duplication == 1).sum()}\")\n",
      "print(f\"Names appearing more than once: {(name_duplication > 1).sum()}\")\n",
      "print(f\"Maximum number of duplications: {name_duplication.max()} (for {name_duplication.idxmax()})\")\n",
      "\n",
      "print(\"\\nExamining duplicate name entries:\")\n",
      "most_common_name = name_duplication.idxmax()\n",
      "duplicates = df[df['Name'] == most_common_name]\n",
      "print(f\"Preferences for '{most_common_name}':\")\n",
      "print(duplicates[['Preferred_City', 'Preferred_Animal', 'Preferred_Thing']].head(5))\n"
     ]
    }
   ],
   "source": [
    "query = \"Within the file 'data.csv', how many individuals with the first name 'Kimberly' have 'Crocodile' as their favourite animal?\"\n\n# Invoke the agent asynchcronously and stream the response\n\nエージェントを非同期で呼び出し、レスポンスをストリーミングする\nresponse_text = \"\"\nasync for event in agent.stream_async(query):\n    if \"data\" in event:\n        # Stream text response\n\nテキストの応答をストリーミングします。\n\nimport asyncio\nimport websockets\n\nasync def hello(websocket, path):\n    name = await websocket.recv()\n    print(f\"< {name}\")\n\n    greeting = f\"Hello {name}!\"\n\n    await websocket.send(greeting)\n    print(f\"> {greeting}\")\n\nstart_server = websockets.serve(hello, \"localhost\", 8765)\n\nasyncio.get_event_loop().run_until_complete(start_server)\nasyncio.get_event_loop().run_forever()\n        chunk = event[\"data\"]\n        response_text += chunk\n        print(chunk, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6b3ce0963d4a83",
   "metadata": {},
   "source": [
    "## 8. クリーンアップ\n\n最後に、Code Interpreter セッションを停止してクリーンアップを行います。セッションの使用が終わったら、リソースを解放し、不要な課金を避けるためにセッションを停止する必要があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "baa2ca7fce8b181d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-13T09:35:31.525724Z",
     "start_time": "2025-07-13T09:35:30.947964Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code Interpreter session stopped successfully!\n"
     ]
    }
   ],
   "source": [
    "# コードインタープリターセッションを停止する\n\n session = PromptSession(cli=cli)\n try:\n     session.app()\n except KeyboardInterrupt:\n     print('無事終了しました')\n finally:\n     session.exit()\ncode_client.stop()\nprint(\"Code Interpreter session stopped successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a01546f53c21a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}